{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":142349,"sourceType":"datasetVersion","datasetId":69692},{"sourceId":1246668,"sourceType":"datasetVersion","datasetId":715814}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-11T13:59:52.767014Z","iopub.execute_input":"2023-12-11T13:59:52.767459Z","iopub.status.idle":"2023-12-11T13:59:52.780457Z","shell.execute_reply.started":"2023-12-11T13:59:52.767427Z","shell.execute_reply":"2023-12-11T13:59:52.779514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import the needed libraries\nimport matplotlib.pyplot as plt \nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nimport emoji\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:52.78549Z","iopub.execute_input":"2023-12-11T13:59:52.786101Z","iopub.status.idle":"2023-12-11T13:59:52.800087Z","shell.execute_reply.started":"2023-12-11T13:59:52.786063Z","shell.execute_reply":"2023-12-11T13:59:52.798828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:52.802479Z","iopub.execute_input":"2023-12-11T13:59:52.802945Z","iopub.status.idle":"2023-12-11T13:59:52.81455Z","shell.execute_reply.started":"2023-12-11T13:59:52.802905Z","shell.execute_reply":"2023-12-11T13:59:52.813376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Upload the training data \ndf_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_train.head()\n#df_train.shape #7613 rows","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:52.816923Z","iopub.execute_input":"2023-12-11T13:59:52.817709Z","iopub.status.idle":"2023-12-11T13:59:52.864537Z","shell.execute_reply.started":"2023-12-11T13:59:52.817667Z","shell.execute_reply":"2023-12-11T13:59:52.863297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"df_train[\"keyword\"].isnull().sum","metadata":{"execution":{"iopub.status.busy":"2023-06-03T14:17:23.908741Z","iopub.execute_input":"2023-06-03T14:17:23.909116Z","iopub.status.idle":"2023-06-03T14:17:23.917384Z","shell.execute_reply.started":"2023-06-03T14:17:23.909088Z","shell.execute_reply":"2023-06-03T14:17:23.916213Z"}}},{"cell_type":"code","source":"#Find all the duplicated Tweets\ndf_duplicated = df_train[df_train.duplicated([\"text\"], keep = False)] #179 rows\ndf_duplicated.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:52.867016Z","iopub.execute_input":"2023-12-11T13:59:52.86826Z","iopub.status.idle":"2023-12-11T13:59:52.885279Z","shell.execute_reply.started":"2023-12-11T13:59:52.868186Z","shell.execute_reply":"2023-12-11T13:59:52.884341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find all the tweets that have been labeled more than once\npd.set_option('display.max_colwidth', None) #to display the whole text\ndf_errors = df_train.groupby('text').filter(lambda x: x['target'].nunique() > 1, display)\ndf_errors.head(20)\n\n#df_errors.shape 55","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:52.88654Z","iopub.execute_input":"2023-12-11T13:59:52.887707Z","iopub.status.idle":"2023-12-11T13:59:53.825593Z","shell.execute_reply.started":"2023-12-11T13:59:52.887673Z","shell.execute_reply":"2023-12-11T13:59:53.824432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Delete all the duplicates\ndf_train = df_train.drop_duplicates(subset='text') #to also remove rows that have same text different target\ndf_train.shape #7503 rows","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:53.827969Z","iopub.execute_input":"2023-12-11T13:59:53.829607Z","iopub.status.idle":"2023-12-11T13:59:53.83949Z","shell.execute_reply.started":"2023-12-11T13:59:53.829569Z","shell.execute_reply":"2023-12-11T13:59:53.838311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Delete all fields with conflicting labels\ndf_train = pd.concat([df_train, df_errors, df_errors]).drop_duplicates(keep=False)\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:53.840782Z","iopub.execute_input":"2023-12-11T13:59:53.84114Z","iopub.status.idle":"2023-12-11T13:59:53.863872Z","shell.execute_reply.started":"2023-12-11T13:59:53.841113Z","shell.execute_reply":"2023-12-11T13:59:53.862555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check that all duplicates are removed\nnull = df_train[\"target\"].isnull().sum()\nprint(null)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:53.86565Z","iopub.execute_input":"2023-12-11T13:59:53.866096Z","iopub.status.idle":"2023-12-11T13:59:53.874082Z","shell.execute_reply.started":"2023-12-11T13:59:53.866055Z","shell.execute_reply":"2023-12-11T13:59:53.87277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check if there are any Nan in the text column\n\nisna = df_train['text'].isna()\nprint(isna)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:53.875698Z","iopub.execute_input":"2023-12-11T13:59:53.876135Z","iopub.status.idle":"2023-12-11T13:59:53.888557Z","shell.execute_reply.started":"2023-12-11T13:59:53.876091Z","shell.execute_reply":"2023-12-11T13:59:53.887324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Text Preprocessing (adapted to clean Twitter text)\nimport string\n\nstop_words = set(stopwords.words('english')) #gets the stopword list from the dedicated library and saves them\ntk = TweetTokenizer() #defines the object, whose method is called in the function\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_text(text):\n        # Check if the input is a string\n    if not isinstance(text, str):\n        # Return non-string input as-is or convert to string\n        return str(text) if text is not None else ''\n    # Remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    # Remove user mentions\n    text = re.sub(r\"@\\S+\", \"\", text)\n    # Remove hashtags\n    text = re.sub(r\"#\\S+\", \"\", text)\n    # Remove punctuation\n    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n    # Remove emojis\n    text = emoji.emojize(text, variant='emoji_type')\n    # Lowercase the text\n    text = text.lower()\n    # Tokenize the text\n    words = tk.tokenize(text)\n    # Remove stop words\n    words = [w for w in words if w not in stop_words]\n    # Join the tokens back together\n    return ' '.join(words)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:53.890188Z","iopub.execute_input":"2023-12-11T13:59:53.890681Z","iopub.status.idle":"2023-12-11T13:59:53.903837Z","shell.execute_reply.started":"2023-12-11T13:59:53.89065Z","shell.execute_reply":"2023-12-11T13:59:53.90255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Apply the preprocessing functon to our text\n\ndf_train['text'] = df_train['text'].apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:53.909456Z","iopub.execute_input":"2023-12-11T13:59:53.90991Z","iopub.status.idle":"2023-12-11T13:59:54.817622Z","shell.execute_reply.started":"2023-12-11T13:59:53.909878Z","shell.execute_reply":"2023-12-11T13:59:54.816396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train['target'].isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:54.818832Z","iopub.execute_input":"2023-12-11T13:59:54.819153Z","iopub.status.idle":"2023-12-11T13:59:54.825228Z","shell.execute_reply.started":"2023-12-11T13:59:54.819126Z","shell.execute_reply":"2023-12-11T13:59:54.823827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As emerging from this plot, the classes are fairly balanced","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\n\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\n# Load GloVe model (example with 100-dimensional model)\nglove_input_file = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'\nword2vec_output_file = 'glove.6B.100d.word2vec'\nglove2word2vec(glove_input_file, word2vec_output_file)\nmodel = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n\n# Define function to create averaged word vector for a text\ndef text_to_vector(text):\n    words = text.split()\n    word_vectors = [model[word] for word in words if word in model]\n    if word_vectors:\n        return np.mean(word_vectors, axis=0)\n    else:\n        return np.zeros(model.vector_size)\n\n# Apply the function to each row in the DataFrame\ndf_train['text_vector'] = df_train['text'].apply(lambda x: text_to_vector(x.lower()))\n\n# Ensure that 'text_vector' is a list of lists (or numpy arrays)\ndf_train['text_vector'] = df_train['text_vector'].apply(lambda x: x if isinstance(x, list) else x.tolist())","metadata":{"execution":{"iopub.status.busy":"2023-12-11T13:59:54.826402Z","iopub.execute_input":"2023-12-11T13:59:54.827101Z","iopub.status.idle":"2023-12-11T14:02:01.022751Z","shell.execute_reply.started":"2023-12-11T13:59:54.827072Z","shell.execute_reply":"2023-12-11T14:02:01.021373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:01.025074Z","iopub.execute_input":"2023-12-11T14:02:01.025582Z","iopub.status.idle":"2023-12-11T14:02:01.052603Z","shell.execute_reply.started":"2023-12-11T14:02:01.02554Z","shell.execute_reply":"2023-12-11T14:02:01.05139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(set(df_train['target']))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:01.053995Z","iopub.execute_input":"2023-12-11T14:02:01.055102Z","iopub.status.idle":"2023-12-11T14:02:01.061882Z","shell.execute_reply.started":"2023-12-11T14:02:01.055061Z","shell.execute_reply":"2023-12-11T14:02:01.060722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\n# Separate features and labels\n\nX = np.array(df_train['text_vector'].tolist())\ny = df_train['target']\n\n\n# Check the shapes of X and y to ensure they are correct\nprint(X.shape)  # Should be (n_samples, n_features)\nprint(y.shape)  # Should be (n_samples,)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:01.063013Z","iopub.execute_input":"2023-12-11T14:02:01.063565Z","iopub.status.idle":"2023-12-11T14:02:01.165057Z","shell.execute_reply.started":"2023-12-11T14:02:01.063533Z","shell.execute_reply":"2023-12-11T14:02:01.163937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking how many labels per class\nlabel_true =  np.sum(y == 1)# 3271\nlabel_false = np.sum(y == 0)\n\n#Plotting the result\nplt.figure(figsize=(5,5))\nplt.bar([\"True\",\"False\"], [label_true, label_false])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:01.166409Z","iopub.execute_input":"2023-12-11T14:02:01.166737Z","iopub.status.idle":"2023-12-11T14:02:01.398406Z","shell.execute_reply.started":"2023-12-11T14:02:01.16671Z","shell.execute_reply":"2023-12-11T14:02:01.397023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train a logistic regression model\nclf = LogisticRegression(solver = 'liblinear')\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:01.399649Z","iopub.execute_input":"2023-12-11T14:02:01.400212Z","iopub.status.idle":"2023-12-11T14:02:01.654265Z","shell.execute_reply.started":"2023-12-11T14:02:01.400169Z","shell.execute_reply":"2023-12-11T14:02:01.652563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:09:27.918764Z","iopub.execute_input":"2023-12-11T14:09:27.919224Z","iopub.status.idle":"2023-12-11T14:09:27.940252Z","shell.execute_reply.started":"2023-12-11T14:09:27.919177Z","shell.execute_reply":"2023-12-11T14:09:27.939038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:08:24.085425Z","iopub.execute_input":"2023-12-11T14:08:24.085854Z","iopub.status.idle":"2023-12-11T14:08:24.095935Z","shell.execute_reply.started":"2023-12-11T14:08:24.085821Z","shell.execute_reply":"2023-12-11T14:08:24.094656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Upload the test data\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:01.675103Z","iopub.execute_input":"2023-12-11T14:02:01.675688Z","iopub.status.idle":"2023-12-11T14:02:01.71693Z","shell.execute_reply.started":"2023-12-11T14:02:01.675648Z","shell.execute_reply":"2023-12-11T14:02:01.715781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:01.718497Z","iopub.execute_input":"2023-12-11T14:02:01.71917Z","iopub.status.idle":"2023-12-11T14:02:01.728452Z","shell.execute_reply.started":"2023-12-11T14:02:01.719127Z","shell.execute_reply":"2023-12-11T14:02:01.727063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preprocess validation data\ndf_test['text'] = df_test['text'].apply(preprocess_text)\ndf_test['text']","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:01.730896Z","iopub.execute_input":"2023-12-11T14:02:01.731921Z","iopub.status.idle":"2023-12-11T14:02:02.194467Z","shell.execute_reply.started":"2023-12-11T14:02:01.731871Z","shell.execute_reply":"2023-12-11T14:02:02.193068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vectorize the validation data\n\ndf_test['text_vector'] = df_test['text'].apply(lambda x: text_to_vector(x.lower()))\n\n# Ensure that 'text_vector' is a list of lists (or numpy arrays)\ndf_test['text_vector'] = df_test['text_vector'].apply(lambda x: x if isinstance(x, list) else x.tolist())\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:02.196151Z","iopub.execute_input":"2023-12-11T14:02:02.19713Z","iopub.status.idle":"2023-12-11T14:02:02.443508Z","shell.execute_reply.started":"2023-12-11T14:02:02.197093Z","shell.execute_reply":"2023-12-11T14:02:02.442175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid = np.array(df_test['text_vector'].tolist())\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:02.44538Z","iopub.execute_input":"2023-12-11T14:02:02.446048Z","iopub.status.idle":"2023-12-11T14:02:02.492071Z","shell.execute_reply.started":"2023-12-11T14:02:02.446003Z","shell.execute_reply":"2023-12-11T14:02:02.490801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict the targets\ny_valid_preds = clf.predict(X_valid)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:02.494018Z","iopub.execute_input":"2023-12-11T14:02:02.497151Z","iopub.status.idle":"2023-12-11T14:02:02.517374Z","shell.execute_reply.started":"2023-12-11T14:02:02.497099Z","shell.execute_reply":"2023-12-11T14:02:02.515422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submit the prediction\noutput = pd.DataFrame({'id': df_test.id, 'target': y_valid_preds})\noutput.to_csv('submission.csv', index=False)\nprint(\"Submission successfully created!\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:02:02.524694Z","iopub.execute_input":"2023-12-11T14:02:02.525779Z","iopub.status.idle":"2023-12-11T14:02:02.553143Z","shell.execute_reply.started":"2023-12-11T14:02:02.525732Z","shell.execute_reply":"2023-12-11T14:02:02.551932Z"},"trusted":true},"execution_count":null,"outputs":[]}]}